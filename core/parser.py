from docx import Document
from pathlib import Path
import re
import json
from loguru import logger
from core.types import Actor, Block, Program

# ============================================================
# üé≠ –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø–∏—Å–∫–∞ –∞–∫—Ç—ë—Ä–æ–≤
# ============================================================

def _load_actor_names() -> set[str]:
    """–ü—Ä–æ–±—É–µ—Ç –Ω–∞–π—Ç–∏ actors_list.json –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö"""
    search_paths = [
        Path(__file__).resolve().parent / "actors_list.json",
        Path(__file__).resolve().parent / "data" / "actors_list.json",
        Path(__file__).resolve().parents[1] / "data" / "actors_list.json",
        Path(__file__).resolve().parents[1] / "actors_list.json",
    ]
    for path in search_paths:
        if path.exists():
            try:
                with open(path, "r", encoding="utf-8") as f:
                    names = {x.strip().lower() for x in json.load(f) if x.strip()}
                    logger.info(f"üé≠ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∞–∫—Ç—ë—Ä–æ–≤: {len(names)} –∏–∑ {path}")
                    return names
            except Exception as e:
                logger.warning(f"‚ö† –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {path}: {e}")
    logger.warning("‚ö† actors_list.json –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî fallback –∫ –±–∞–∑–æ–≤–æ–º—É –ø–∞—Ä—Å–∏–Ω–≥—É.")
    return set()


ACTOR_NAMES = _load_actor_names()

# ============================================================
# üß© –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
# ============================================================

_SPLIT_RE = re.compile(r"[\n\r\u000b\u2028\u2029;,/\\]+")


def _split_people_blob(blob: str) -> list[str]:
    """–†–∞–∑–¥–µ–ª—è–µ—Ç —Å—Ç—Ä–æ–∫—É –∞–∫—Ç—ë—Ä–æ–≤ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã"""
    if not blob:
        return []
    return [p.strip() for p in _SPLIT_RE.split(blob) if p.strip()]


def _clean_actor_token(token: str) -> str:
    """–£–¥–∞–ª—è–µ—Ç –º—É—Å–æ—Ä–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –∏–º—è"""
    return re.sub(r"[%!\d.,]+", "", token).strip()


def _try_split_concatenated(token: str) -> list[str]:
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Å–∫–ª–µ–µ–Ω–Ω—ã–µ –∏–º–µ–Ω–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä '–ò–ª–∞–Ω–∞–ö—Å—é—à–∞') –ø–æ —Å–ª–æ–≤–∞—Ä—é –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∞–∫—Ç—ë—Ä–æ–≤"""
    if not ACTOR_NAMES:
        return [token]
    low = token.lower()
    found = []
    i = 0
    while i < len(low):
        match = None
        for name in sorted(ACTOR_NAMES, key=len, reverse=True):
            if low.startswith(name, i):
                found.append(name)
                i += len(name)
                match = True
                break
        if not match:
            i += 1
    if len(found) > 1:
        return [n.capitalize() for n in found]
    return [token]


def parse_actors(raw: str) -> list[Actor]:
    """–ü–∞—Ä—Å–∏—Ç —Å–ø–∏—Å–æ–∫ –∞–∫—Ç—ë—Ä–æ–≤ –∏ –∏—Ö —Ç–µ–≥–∏"""
    if not raw:
        return []

    result: list[Actor] = []
    for token in _split_people_blob(raw):
        if not token.strip():
            continue

        tags = set()
        name = token.strip()

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–≥–∏
        if "%" in name:
            tags.add("later")
        if "!" in name:
            tags.add("early")
        if re.search(r"\(?\b–≥\s*–∫\b\)?", name, flags=re.IGNORECASE):
            tags.add("gk")

        # –ß–∏—Å—Ç–∏–º –∏–º—è
        name = re.sub(r"\(?\b–≥\s*–∫\b\)?", "", name, flags=re.IGNORECASE)
        name = _clean_actor_token(name)

        # –†–∞–∑–±–∏–≤–∞–µ–º —Å–∫–ª–µ–µ–Ω–Ω—ã–µ –∏–º–µ–Ω–∞
        for nm in _try_split_concatenated(name):
            nm = " ".join(nm.split())
            if nm:
                result.append(Actor(name=nm, tags=sorted(list(tags))))

    return result

# ============================================================
# üìò –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞—Ä—Å–µ—Ä –ø—Ä–æ–≥—Ä–∞–º–º—ã
# ============================================================

def parse_docx(path: str) -> Program:
    """–ß–∏—Ç–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—É .docx –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç Program"""
    logger.info(f"üìÑ –ß—Ç–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {path}")
    doc = Document(path)
    if not doc.tables:
        logger.error("‚ùå –í –¥–æ–∫—É–º–µ–Ω—Ç–µ –Ω–µ—Ç —Ç–∞–±–ª–∏—Ü.")
        return Program(blocks=[])

    table = doc.tables[0]
    blocks: list[Block] = []

    for i, row in enumerate(table.rows[1:], start=1):
        texts = [cell.text.strip() for cell in row.cells]
        if not any(texts):
            continue

        num = texts[0] if len(texts) > 0 else ""
        title = texts[1] if len(texts) > 1 else ""
        actors_raw = texts[2] if len(texts) > 2 else ""
        pp_raw = texts[3] if len(texts) > 3 else ""

        main_actors = parse_actors(actors_raw)
        pp_actors = parse_actors(pp_raw)

        # –°–ª–∏–≤–∞–µ–º –∞–∫—Ç—ë—Ä–æ–≤ –∏ –∏—Ö —Ç–µ–≥–∏
        merged = {a.name: set(a.tags) for a in main_actors}
        for pa in pp_actors:
            merged.setdefault(pa.name, set()).update(pa.tags)

        actors = [Actor(name=k, tags=sorted(v)) for k, v in merged.items()]

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –±–ª–æ–∫–∞
        block_type = "–æ–±—ã—á–Ω—ã–π"
        lt = title.lower()
        if "–ø—Ä–µ–¥–∫—É–ª–∏—Å—å–µ" in lt:
            block_type = "–ø—Ä–µ–¥–∫—É–ª–∏—Å—å–µ"
        elif "—Å–ø–æ–Ω—Å–æ—Ä" in lt:
            block_type = "—Å–ø–æ–Ω—Å–æ—Ä—ã"
        elif "—Ç—è–Ω—É—á" in lt:
            block_type = "—Ç—è–Ω—É—á–∫–∞"

        block = Block(
            index=i,
            pp=pp_raw,
            actors=actors,
            description=title,
            type=block_type
        )
        blocks.append(block)

    logger.info(f"‚úÖ –ü—Ä–æ—á–∏—Ç–∞–Ω–æ –±–ª–æ–∫–æ–≤: {len(blocks)}")
    return Program(blocks=blocks)
